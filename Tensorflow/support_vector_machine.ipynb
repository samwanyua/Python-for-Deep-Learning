{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machine (SVM)\n",
    "\n",
    "#### Introduction  \n",
    "Support Vector Machine (SVM) is a supervised learning algorithm used for classification and regression tasks. It is particularly effective for high-dimensional spaces and works well when the number of dimensions exceeds the number of samples.\n",
    "\n",
    "#### How SVM Works  \n",
    "SVM finds the optimal **hyperplane** that best separates data points into different classes. The hyperplane is chosen to maximize the **margin**, which is the distance between the nearest points (support vectors) of each class.\n",
    "\n",
    "#### Key Concepts  \n",
    "1. **Hyperplane**: A decision boundary that separates different classes.  \n",
    "2. **Support Vectors**: Data points closest to the hyperplane that influence its position.  \n",
    "3. **Margin**: The distance between the hyperplane and the nearest support vectors. SVM aims to maximize this margin for better generalization.  \n",
    "\n",
    "### Types of SVM  \n",
    "### 1. **Linear SVM**  \n",
    "   - Used when data is **linearly separable**.\n",
    "   - Finds the straight-line (or hyperplane in higher dimensions) that separates classes.  \n",
    "   \n",
    "   **Equation of the hyperplane:**  \n",
    "   \\[\n",
    "   w \\cdot x + b = 0\n",
    "   \\]\n",
    "   where:\n",
    "   - \\( w \\) is the weight vector,\n",
    "   - \\( x \\) is the feature vector,\n",
    "   - \\( b \\) is the bias term.\n",
    "\n",
    "### 2. **Non-Linear SVM (Using Kernels)**  \n",
    "   - Used when data is **not linearly separable**.\n",
    "   - Maps data into a higher-dimensional space where it becomes linearly separable.\n",
    "   - Uses kernel functions such as:\n",
    "     - **Polynomial Kernel**: \\( (x \\cdot x')^d \\)\n",
    "     - **Radial Basis Function (RBF) Kernel**: \\( e^{-\\gamma ||x - x'||^2} \\)\n",
    "     - **Sigmoid Kernel**: \\( \\tanh(\\alpha x \\cdot x' + c) \\)\n",
    "\n",
    "### Hyperparameters of SVM  \n",
    "1. **C (Regularization Parameter)**  \n",
    "   - Controls the trade-off between achieving a low error and maximizing the margin.\n",
    "   - A **high C** results in a smaller margin but fewer misclassifications.\n",
    "   - A **low C** results in a larger margin but more misclassifications.\n",
    "\n",
    "2. **Gamma (for RBF Kernel)**  \n",
    "   - Defines how much influence a single training example has.\n",
    "   - **High gamma** → Closer decision boundary, risk of overfitting.\n",
    "   - **Low gamma** → Smoother decision boundary, risk of underfitting.\n",
    "\n",
    "## Advantages of SVM  \n",
    "- Effective in high-dimensional spaces.  \n",
    "- Works well for both linear and non-linear data (using kernels).  \n",
    "- Robust to overfitting in high-dimensional datasets.  \n",
    "- Memory efficient, as it uses a subset of training points (support vectors).  \n",
    "\n",
    "## Disadvantages of SVM  \n",
    "- Can be slow on large datasets.  \n",
    "- Sensitive to the choice of kernel and hyperparameters.  \n",
    "- Not ideal when the dataset has significant overlapping classes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
