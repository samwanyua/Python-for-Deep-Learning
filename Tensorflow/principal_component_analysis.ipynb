{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Principal Component Analysis (PCA)**\n",
    "\n",
    "### **What is PCA?**  \n",
    "Principal Component Analysis (PCA) is a **dimensionality reduction technique** used in machine learning and data analysis. It helps in transforming high-dimensional data into a lower-dimensional form while preserving as much variance as possible.\n",
    "\n",
    "---\n",
    "\n",
    "### **Key Goals of PCA**  \n",
    "1. **Reduce the number of features (dimensions)** while retaining important information.  \n",
    "2. **Remove redundancy and correlation** between features.  \n",
    "3. **Improve computational efficiency** for large datasets.  \n",
    "4. **Visualize high-dimensional data** in 2D or 3D.  \n",
    "\n",
    "---\n",
    "\n",
    "### **How PCA Works (Step-by-Step)**  \n",
    "\n",
    "1. **Standardization of Data**  \n",
    "   - Convert all features to have a mean of 0 and unit variance (important when features have different scales).  \n",
    "\n",
    "2. **Compute the Covariance Matrix**  \n",
    "   - Measures how different features vary together.  \n",
    "\n",
    "3. **Compute Eigenvectors and Eigenvalues**  \n",
    "   - Eigenvectors represent the **principal components** (directions of maximum variance).  \n",
    "   - Eigenvalues indicate the **importance** (variance captured) by each principal component.  \n",
    "\n",
    "4. **Select Principal Components**  \n",
    "   - Keep the top **k** components that explain most of the variance.  \n",
    "\n",
    "5. **Transform the Data**  \n",
    "   - Project original data onto the selected principal components.  \n",
    "\n",
    "---\n",
    "\n",
    "### **Visualization of PCA**  \n",
    "- PCA projects data onto a new set of **orthogonal axes**, capturing **maximum variance** in the first few principal components.  \n",
    "- Often, **2D or 3D PCA plots** help visualize high-dimensional data.\n",
    "\n",
    "---\n",
    "\n",
    "### **Advantages of PCA**  \n",
    "✔ **Reduces Dimensionality** → Makes datasets more manageable.  \n",
    "✔ **Removes Correlation** → Features become independent.  \n",
    "✔ **Improves Model Performance** → Less complexity and faster training.  \n",
    "✔ **Useful for Visualization** → Helps interpret data in lower dimensions.  \n",
    "\n",
    "---\n",
    "\n",
    "### **Disadvantages of PCA**  \n",
    "- **Loss of Information** → Some variance is lost during dimensionality reduction.  \n",
    "- **Difficult to Interpret** → Principal components are linear combinations of features, making them hard to understand.  \n",
    "- **Assumes Linearity** → PCA works best when data has linear relationships.  \n",
    "- **Sensitive to Scaling** → Requires proper standardization of features.  \n",
    "\n",
    "---\n",
    "\n",
    "### **When to Use PCA?**  \n",
    "- When you have **high-dimensional data** (many features).  \n",
    "- When you want to **remove redundancy and correlation** in features.  \n",
    "- When you need **faster training** for machine learning models.  \n",
    "- When visualizing **complex datasets** in 2D or 3D.  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
