{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "\n",
    "#### Introduction\n",
    "Logistic Regression is a **supervised learning algorithm** used for **classification tasks**. It predicts **probabilities** and is commonly used in **binary classification problems** (e.g., spam detection, disease prediction).\n",
    "\n",
    "#### How It Works\n",
    "Instead of predicting a continuous value (like Linear Regression), Logistic Regression predicts the **probability (p) that an instance belongs to a certain class** using the **sigmoid function**:\n",
    "\n",
    "\\[\n",
    "p = \\frac{1}{1 + e^{-(wX + b)}}\n",
    "\\]\n",
    "\n",
    "Where:\n",
    "- \\( wX + b \\) is the **linear equation** (weights and bias),\n",
    "- \\( e \\) is Euler’s number (~2.718),\n",
    "- The result is between **0 and 1**, interpreted as a probability.\n",
    "\n",
    "#### Decision Boundary\n",
    "- If \\( p \\geq 0.5 \\), classify as **1** (positive class).\n",
    "- If \\( p < 0.5 \\), classify as **0** (negative class).\n",
    "\n",
    "This forms a **decision boundary** that separates different classes.\n",
    "\n",
    "#### Cost Function (Log Loss)\n",
    "Logistic Regression uses **Log Loss (Binary Cross-Entropy)** to measure how well the model predicts:\n",
    "\n",
    "\\[\n",
    "J(w, b) = -\\frac{1}{m} \\sum \\left[ y \\log(\\hat{y}) + (1 - y) \\log(1 - \\hat{y}) \\right]\n",
    "\\]\n",
    "\n",
    "Where:\n",
    "- \\( y \\) is the actual class (0 or 1),\n",
    "- \\( \\hat{y} \\) is the predicted probability,\n",
    "- \\( m \\) is the number of samples.\n",
    "\n",
    "#### Types of Logistic Regression\n",
    "- **Binary Logistic Regression** → Two classes (e.g., spam or not spam).\n",
    "- **Multiclass Logistic Regression** → Multiple classes using **One-vs-Rest (OvR)** or **Softmax Regression**.\n",
    "\n",
    "#### Advantages\n",
    "- Simple and efficient for classification  \n",
    "- Interpretable and easy to implement  \n",
    "- Works well with **linearly separable** data  \n",
    "\n",
    "#### Limitations\n",
    "- Struggles with **non-linear** relationships (requires feature engineering or a more complex model).  \n",
    "- Sensitive to **outliers**.  \n",
    "- Assumes **independent features** (may require preprocessing).  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
