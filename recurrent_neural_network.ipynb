{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recurrent Neural Networks (RNNs) - Explained\n",
    "\n",
    "### What is an RNN?\n",
    "A **Recurrent Neural Network (RNN)** is a type of neural network that is designed to handle **sequential data**, such as **time series, speech, text, and video**. Unlike traditional neural networks, RNNs have **memory** that enables them to process and learn from previous inputs.\n",
    "\n",
    "---\n",
    "\n",
    "###  **Architecture of an RNN**\n",
    "### **1Ô∏è Standard Neural Network vs. RNN**\n",
    "| Feature            | Standard Neural Network | Recurrent Neural Network |\n",
    "|--------------------|------------------------|--------------------------|\n",
    "| Input Type        | Independent Data Points | Sequential Data (Time-dependent) |\n",
    "| Memory            | No memory               | Has memory (stores past information) |\n",
    "| Suitable For      | Image Classification    | Text, Speech, Time-Series Data |\n",
    "\n",
    "### **2Ô∏è RNN Structure**\n",
    "- **Recurrent Connection:** Unlike a feedforward network, an RNN loops back on itself, allowing information to persist.\n",
    "- **Hidden State:** Maintains information from previous time steps.\n",
    "- **Mathematical Representation:**\n",
    "  \\[\n",
    "  h_t = f(W_h h_{t-1} + W_x X_t + b)\n",
    "  \\]\n",
    "  where:\n",
    "  - \\( h_t \\) = current hidden state\n",
    "  - \\( X_t \\) = current input\n",
    "  - \\( W_h \\), \\( W_x \\) = weight matrices\n",
    "  - \\( b \\) = bias\n",
    "  - \\( f \\) = activation function (e.g., **tanh**)\n",
    "\n",
    "---\n",
    "\n",
    "###  **How an RNN Works**\n",
    "###  **Example: Predicting the next word in a sentence**\n",
    "1Ô∏è‚É£ **Input:** \"The cat sat on the\" ‚Üí Predicts \"mat\"  \n",
    "2Ô∏è‚É£ **Processing:** Each word is fed sequentially into the RNN.  \n",
    "3Ô∏è‚É£ **Memory:** Hidden states store past word relationships.  \n",
    "4Ô∏è‚É£ **Output:** The network predicts the next word.  \n",
    "\n",
    "---\n",
    "\n",
    "###  **Types of RNN Architectures**\n",
    "### **1Ô∏è‚É£ Many-to-One (Sentiment Analysis)**\n",
    "- Example: Given a **sentence**, predict a **sentiment** (Positive/Negative).\n",
    "- Input: `\"I love this movie\"` ‚Üí Output:  (Positive)\n",
    "\n",
    "### **2Ô∏è‚É£ One-to-Many (Music Generation)**\n",
    "- Example: Given a **note**, generate an entire **music sequence**.\n",
    "- Input: üéµ ‚Üí Output: üé∂üé∂üé∂\n",
    "\n",
    "### **3Ô∏è‚É£ Many-to-Many (Machine Translation)**\n",
    "- Example: Translating English to French.\n",
    "- Input: `\"Hello, how are you?\"`\n",
    "- Output: `\"Bonjour, comment √ßa va?\"`\n",
    "\n",
    "---\n",
    "\n",
    "### **Challenges in RNNs**\n",
    "### üîπ **1. Vanishing Gradient Problem**\n",
    "- When backpropagating through time, gradients **become too small**, causing earlier layers to learn very slowly.\n",
    "- **Solution:** Use **LSTMs or GRUs** instead of vanilla RNNs.\n",
    "\n",
    "### üîπ **2. Exploding Gradient Problem**\n",
    "- Gradients **become too large**, making the model unstable.\n",
    "- **Solution:** Apply **gradient clipping**.\n",
    "\n",
    "---\n",
    "\n",
    "### **LSTMs (Long Short-Term Memory)**\n",
    "LSTMs are an advanced type of RNN that solves the **vanishing gradient problem** by introducing **gates**:\n",
    "1Ô∏è‚É£ **Forget Gate:** Decides what information to discard.  \n",
    "2Ô∏è‚É£ **Input Gate:** Determines which new information to add.  \n",
    "3Ô∏è‚É£ **Output Gate:** Controls the final output.  \n",
    "\n",
    "###  **LSTM Cell Diagram**\n",
    "‚¨ú Input (Xt) ‚Üí üü® Forget Gate ‚Üí üü© Input Gate ‚Üí üîµ Output Gate ‚Üí üü† Hidden State\n",
    "\n",
    "\n",
    "### Applications of RNNs\n",
    "-Speech Recognition (e.g., Siri, Google Assistant)\n",
    "- Machine Translation (e.g., Google Translate)\n",
    "- Stock Market Prediction (e.g., Forecasting Prices)\n",
    "- Chatbots & Conversational AI (e.g., ChatGPT)\n",
    "- Music & Text Generation (e.g., AI-generated stories, songs)\n",
    "\n",
    "### Summary\n",
    "- RNNs process sequential data by using hidden states.\n",
    "- They suffer from vanishing gradients, which LSTMs and GRUs solve.\n",
    "- Used in speech, text, finance, and forecasting tasks.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
