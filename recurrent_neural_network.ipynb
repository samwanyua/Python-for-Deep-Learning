{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recurrent Neural Networks (RNNs) - Explained\n",
    "\n",
    "### What is an RNN?\n",
    "A **Recurrent Neural Network (RNN)** is a type of neural network that is designed to handle **sequential data**, such as **time series, speech, text, and video**. Unlike traditional neural networks, RNNs have **memory** that enables them to process and learn from previous inputs.\n",
    "\n",
    "---\n",
    "\n",
    "###  **Architecture of an RNN**\n",
    "### **1Ô∏è Standard Neural Network vs. RNN**\n",
    "| Feature            | Standard Neural Network | Recurrent Neural Network |\n",
    "|--------------------|------------------------|--------------------------|\n",
    "| Input Type        | Independent Data Points | Sequential Data (Time-dependent) |\n",
    "| Memory            | No memory               | Has memory (stores past information) |\n",
    "| Suitable For      | Image Classification    | Text, Speech, Time-Series Data |\n",
    "\n",
    "### **2Ô∏è RNN Structure**\n",
    "- **Recurrent Connection:** Unlike a feedforward network, an RNN loops back on itself, allowing information to persist.\n",
    "- **Hidden State:** Maintains information from previous time steps.\n",
    "- **Mathematical Representation:**\n",
    "  \\[\n",
    "  h_t = f(W_h h_{t-1} + W_x X_t + b)\n",
    "  \\]\n",
    "  where:\n",
    "  - \\( h_t \\) = current hidden state\n",
    "  - \\( X_t \\) = current input\n",
    "  - \\( W_h \\), \\( W_x \\) = weight matrices\n",
    "  - \\( b \\) = bias\n",
    "  - \\( f \\) = activation function (e.g., **tanh**)\n",
    "\n",
    "---\n",
    "\n",
    "###  **How an RNN Works**\n",
    "###  **Example: Predicting the next word in a sentence**\n",
    "1Ô∏è‚É£ **Input:** \"The cat sat on the\" ‚Üí Predicts \"mat\"  \n",
    "2Ô∏è‚É£ **Processing:** Each word is fed sequentially into the RNN.  \n",
    "3Ô∏è‚É£ **Memory:** Hidden states store past word relationships.  \n",
    "4Ô∏è‚É£ **Output:** The network predicts the next word.  \n",
    "\n",
    "---\n",
    "\n",
    "###  **Types of RNN Architectures**\n",
    "### **1Ô∏è‚É£ Many-to-One (Sentiment Analysis)**\n",
    "- Example: Given a **sentence**, predict a **sentiment** (Positive/Negative).\n",
    "- Input: `\"I love this movie\"` ‚Üí Output:  (Positive)\n",
    "\n",
    "### **2Ô∏è‚É£ One-to-Many (Music Generation)**\n",
    "- Example: Given a **note**, generate an entire **music sequence**.\n",
    "- Input: üéµ ‚Üí Output: üé∂üé∂üé∂\n",
    "\n",
    "### **3Ô∏è‚É£ Many-to-Many (Machine Translation)**\n",
    "- Example: Translating English to French.\n",
    "- Input: `\"Hello, how are you?\"`\n",
    "- Output: `\"Bonjour, comment √ßa va?\"`\n",
    "\n",
    "---\n",
    "\n",
    "### **Challenges in RNNs**\n",
    "### üîπ **1. Vanishing Gradient Problem**\n",
    "- When backpropagating through time, gradients **become too small**, causing earlier layers to learn very slowly.\n",
    "- **Solution:** Use **LSTMs or GRUs** instead of vanilla RNNs.\n",
    "\n",
    "### üîπ **2. Exploding Gradient Problem**\n",
    "- Gradients **become too large**, making the model unstable.\n",
    "- **Solution:** Apply **gradient clipping**.\n",
    "\n",
    "---\n",
    "\n",
    "### **LSTMs (Long Short-Term Memory)**\n",
    "LSTMs are an advanced type of RNN that solves the **vanishing gradient problem** by introducing **gates**:\n",
    "1Ô∏è‚É£ **Forget Gate:** Decides what information to discard.  \n",
    "2Ô∏è‚É£ **Input Gate:** Determines which new information to add.  \n",
    "3Ô∏è‚É£ **Output Gate:** Controls the final output.  \n",
    "\n",
    "###  **LSTM Cell Diagram**\n",
    "‚¨ú Input (Xt) ‚Üí üü® Forget Gate ‚Üí üü© Input Gate ‚Üí üîµ Output Gate ‚Üí üü† Hidden State\n",
    "\n",
    "\n",
    "### Applications of RNNs\n",
    "-Speech Recognition (e.g., Siri, Google Assistant)\n",
    "- Machine Translation (e.g., Google Translate)\n",
    "- Stock Market Prediction (e.g., Forecasting Prices)\n",
    "- Chatbots & Conversational AI (e.g., ChatGPT)\n",
    "- Music & Text Generation (e.g., AI-generated stories, songs)\n",
    "\n",
    "### Summary\n",
    "- RNNs process sequential data by using hidden states.\n",
    "- They suffer from vanishing gradients, which LSTMs and GRUs solve.\n",
    "- Used in speech, text, finance, and forecasting tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras import layers, models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
      "17464789/17464789 [==============================] - 13s 1us/step\n"
     ]
    }
   ],
   "source": [
    "# Load and preprocess data\n",
    "max_features = 10000\n",
    "max_len = 500\n",
    "(X_train,y_train),(X_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_len)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, None, 32)          320000    \n",
      "                                                                 \n",
      " simple_rnn (SimpleRNN)      (None, 32)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 322113 (1.23 MB)\n",
      "Trainable params: 322113 (1.23 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Build model\n",
    "model = models.Sequential()\n",
    "model.add(layers.Embedding(max_features, 32))\n",
    "model.add(layers.SimpleRNN(32))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-27 15:28:24.230937: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 40000000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "157/157 [==============================] - 65s 387ms/step - loss: 0.6257 - accuracy: 0.6349 - val_loss: 0.4673 - val_accuracy: 0.8026\n",
      "Epoch 2/10\n",
      "157/157 [==============================] - 62s 392ms/step - loss: 0.4021 - accuracy: 0.8285 - val_loss: 0.3720 - val_accuracy: 0.8458\n",
      "Epoch 3/10\n",
      "157/157 [==============================] - 61s 392ms/step - loss: 0.3104 - accuracy: 0.8754 - val_loss: 0.3671 - val_accuracy: 0.8514\n",
      "Epoch 4/10\n",
      "157/157 [==============================] - 47s 300ms/step - loss: 0.2454 - accuracy: 0.9053 - val_loss: 0.3532 - val_accuracy: 0.8624\n",
      "Epoch 5/10\n",
      "157/157 [==============================] - 23s 145ms/step - loss: 0.1888 - accuracy: 0.9291 - val_loss: 0.5823 - val_accuracy: 0.7968\n",
      "Epoch 6/10\n",
      "157/157 [==============================] - 25s 162ms/step - loss: 0.1292 - accuracy: 0.9536 - val_loss: 0.7581 - val_accuracy: 0.7622\n",
      "Epoch 7/10\n",
      "157/157 [==============================] - 28s 177ms/step - loss: 0.1707 - accuracy: 0.9414 - val_loss: 0.4738 - val_accuracy: 0.8420\n",
      "Epoch 8/10\n",
      "157/157 [==============================] - 25s 158ms/step - loss: 0.0665 - accuracy: 0.9799 - val_loss: 0.5171 - val_accuracy: 0.8420\n",
      "Epoch 9/10\n",
      "157/157 [==============================] - 23s 148ms/step - loss: 0.0531 - accuracy: 0.9848 - val_loss: 0.5786 - val_accuracy: 0.8248\n",
      "Epoch 10/10\n",
      "157/157 [==============================] - 22s 137ms/step - loss: 0.0358 - accuracy: 0.9890 - val_loss: 0.6592 - val_accuracy: 0.8078\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7f5aebe65820>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compile and train the model\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=128, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  3/782 [..............................] - ETA: 22s - loss: 0.6996 - accuracy: 0.8333"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-27 15:34:51.971803: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 50000000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 20s 25ms/step - loss: 0.6706 - accuracy: 0.8086\n",
      "Test accuracy: 0.8086400032043457\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "test_loss, test_acc = model.evaluate(X_test,y_test)\n",
    "print(f\"Test accuracy: {test_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
