{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recurrent Neural Networks (RNNs) - Explained\n",
    "\n",
    "### What is an RNN?\n",
    "A **Recurrent Neural Network (RNN)** is a type of neural network that is designed to handle **sequential data**, such as **time series, speech, text, and video**. Unlike traditional neural networks, RNNs have **memory** that enables them to process and learn from previous inputs.\n",
    "\n",
    "---\n",
    "\n",
    "###  **Architecture of an RNN**\n",
    "### **1Ô∏è Standard Neural Network vs. RNN**\n",
    "| Feature            | Standard Neural Network | Recurrent Neural Network |\n",
    "|--------------------|------------------------|--------------------------|\n",
    "| Input Type        | Independent Data Points | Sequential Data (Time-dependent) |\n",
    "| Memory            | No memory               | Has memory (stores past information) |\n",
    "| Suitable For      | Image Classification    | Text, Speech, Time-Series Data |\n",
    "\n",
    "### **2Ô∏è RNN Structure**\n",
    "- **Recurrent Connection:** Unlike a feedforward network, an RNN loops back on itself, allowing information to persist.\n",
    "- **Hidden State:** Maintains information from previous time steps.\n",
    "- **Mathematical Representation:**\n",
    "  \\[\n",
    "  h_t = f(W_h h_{t-1} + W_x X_t + b)\n",
    "  \\]\n",
    "  where:\n",
    "  - \\( h_t \\) = current hidden state\n",
    "  - \\( X_t \\) = current input\n",
    "  - \\( W_h \\), \\( W_x \\) = weight matrices\n",
    "  - \\( b \\) = bias\n",
    "  - \\( f \\) = activation function (e.g., **tanh**)\n",
    "\n",
    "---\n",
    "\n",
    "###  **How an RNN Works**\n",
    "###  **Example: Predicting the next word in a sentence**\n",
    "1Ô∏è‚É£ **Input:** \"The cat sat on the\" ‚Üí Predicts \"mat\"  \n",
    "2Ô∏è‚É£ **Processing:** Each word is fed sequentially into the RNN.  \n",
    "3Ô∏è‚É£ **Memory:** Hidden states store past word relationships.  \n",
    "4Ô∏è‚É£ **Output:** The network predicts the next word.  \n",
    "\n",
    "---\n",
    "\n",
    "###  **Types of RNN Architectures**\n",
    "### **1Ô∏è‚É£ Many-to-One (Sentiment Analysis)**\n",
    "- Example: Given a **sentence**, predict a **sentiment** (Positive/Negative).\n",
    "- Input: `\"I love this movie\"` ‚Üí Output:  (Positive)\n",
    "\n",
    "### **2Ô∏è‚É£ One-to-Many (Music Generation)**\n",
    "- Example: Given a **note**, generate an entire **music sequence**.\n",
    "- Input: üéµ ‚Üí Output: üé∂üé∂üé∂\n",
    "\n",
    "### **3Ô∏è‚É£ Many-to-Many (Machine Translation)**\n",
    "- Example: Translating English to French.\n",
    "- Input: `\"Hello, how are you?\"`\n",
    "- Output: `\"Bonjour, comment √ßa va?\"`\n",
    "\n",
    "---\n",
    "\n",
    "### **Challenges in RNNs**\n",
    "### üîπ **1. Vanishing Gradient Problem**\n",
    "- When backpropagating through time, gradients **become too small**, causing earlier layers to learn very slowly.\n",
    "- **Solution:** Use **LSTMs or GRUs** instead of vanilla RNNs.\n",
    "\n",
    "### üîπ **2. Exploding Gradient Problem**\n",
    "- Gradients **become too large**, making the model unstable.\n",
    "- **Solution:** Apply **gradient clipping**.\n",
    "\n",
    "---\n",
    "\n",
    "### **LSTMs (Long Short-Term Memory)**\n",
    "LSTMs are an advanced type of RNN that solves the **vanishing gradient problem** by introducing **gates**:\n",
    "1Ô∏è‚É£ **Forget Gate:** Decides what information to discard.  \n",
    "2Ô∏è‚É£ **Input Gate:** Determines which new information to add.  \n",
    "3Ô∏è‚É£ **Output Gate:** Controls the final output.  \n",
    "\n",
    "###  **LSTM Cell Diagram**\n",
    "‚¨ú Input (Xt) ‚Üí üü® Forget Gate ‚Üí üü© Input Gate ‚Üí üîµ Output Gate ‚Üí üü† Hidden State\n",
    "\n",
    "\n",
    "### Applications of RNNs\n",
    "-Speech Recognition (e.g., Siri, Google Assistant)\n",
    "- Machine Translation (e.g., Google Translate)\n",
    "- Stock Market Prediction (e.g., Forecasting Prices)\n",
    "- Chatbots & Conversational AI (e.g., ChatGPT)\n",
    "- Music & Text Generation (e.g., AI-generated stories, songs)\n",
    "\n",
    "### Summary\n",
    "- RNNs process sequential data by using hidden states.\n",
    "- They suffer from vanishing gradients, which LSTMs and GRUs solve.\n",
    "- Used in speech, text, finance, and forecasting tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras import layers, models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
      "17464789/17464789 [==============================] - 13s 1us/step\n"
     ]
    }
   ],
   "source": [
    "# Load and preprocess data\n",
    "max_features = 10000\n",
    "max_len = 500\n",
    "(X_train,y_train),(X_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_len)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, None, 32)          320000    \n",
      "                                                                 \n",
      " simple_rnn (SimpleRNN)      (None, 32)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 322113 (1.23 MB)\n",
      "Trainable params: 322113 (1.23 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Build model\n",
    "model = models.Sequential()\n",
    "model.add(layers.Embedding(max_features, 32))\n",
    "model.add(layers.SimpleRNN(32))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
